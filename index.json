[{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/swarm/create_swarm/","title":"Créer son cluster Swarm","tags":[],"description":"","content":"Activation Swarm n\u0026rsquo;est pas actif par defaut dans Docker.\nEssayer la commande suivante:\n$ docker node ls Le premier noeud d\u0026rsquo;un cluster Swarm est initialisé avec docker swarm init\nNe pas executer docker swarm init sur tous les noeuds !\nVous auriez plusieurs clusters différents.\n  Pour commencer il faut bien nommer ses machines.\nVérifier que les VMs ont bien 3 noms distincts, sinon renommer les !\n$ sudo hostnamectl set-hostname node1 $ sudo hostnamectl set-hostname node2 $ sudo hostnamectl set-hostname node3 Puis vérifier qu\u0026rsquo;ils sont bien synchronisé à un serveur de temps\nRaft est très sensible sur les timing\n$ sudo timedatectl set-timezone Europe/Paris $ sudo timedatectl set-ntp true  Maintenant allons créer notre cluster sur le node1\n$ docker swarm init  Il est possible de choisir son interface de control(\u0026ndash;advertise-addr/\u0026ndash;listen-addr) et de data(\u0026ndash;data-path-addr)\nLe control plane est utilisé par Swarm pour les communications manager/worker, election Raft,\u0026hellip; Le data plane est utilisé pour les communications entre les conteneurs.\n Les tokens Docker à générer deux tokens pour notre cluster, un pour joindre les managers et l\u0026rsquo;autre pour joindre les workers. Vous en avez aperçu un juste après l\u0026rsquo;initialisation.\nTo add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-29jzjk0kmcunwcxlweugx75jhe0iqgokj2hdfkrad2ebrv640t-dsup5h5kmp5o7fndkk05zprkm 10.59.72.14:2377 Verifier que nous avons bien activé Swarm\n$ docker info [...] Swarm: active [...] Ressayons la commande de tout a l\u0026rsquo;heure\n$ docker node ls Ajouter un noeud Un cluster avec noeud c\u0026rsquo;est pas fun. Ajoutons node2 à notre cluster en tant que worker. Sur le node1\n// Afficher le token des worker $ docker swarm join-token worker Puis executer la commande affichée en sortie sur node2\n$ ssh node2 $ docker swarm join ... Restons un peu sur ce node2 Verifions que la commande à bien activé Swarm\n$ docker info | grep Swarm Cependant, les commandes Swarm ne fonctionneront pas\n$ docker node ls Nous sommes sur node2, un worker, seuls les managers peuvent recevoir les commandes de cluster.\nRetournons sur node1 et voyons a quoi ressemble notre cluster maintenant.\n$ docker node ls  Les tokens sont générés à l\u0026rsquo;initialisation du cluster, ce sont des certificats signés par le CA du cluster.\nOn peut les regénérer avec docker swarm join-token --rotate \u0026lt;worker|manager\u0026gt; si ils sont compromis.\n Le control plance est crypté, les clefs sont regénérés toutes les 12h.\nLes certificats eux sont valable 90 jours par défaut.\nLe data plane, lui, n\u0026rsquo;est pas crypté par défaut mais peut être activé pas réseau.\n Ajouter un autre manager Nous avons donc un manager node1 et un worker node2.\nSi on perd node1, on perd le quorum du Raft et c\u0026rsquo;est mal.\nLes services continueront de fonctionner, mais plus aucune commande cluster ne sera accepté.\nSi le manager ne revient pas, il va falloir faire une réparation manuelle, personne ne veut ça.\nAllons ajouter le node3 en tant que manager.\n// Sur un manager $ docker docker swarm join-token manager // Sur notre node3 $ docker swarm join --token ...  Essayer la commande docker node ls sur le node3 L\u0026rsquo;étoile (*) à coté de l\u0026rsquo;ID du neud correspond au manager auquel nous sommes connectés.\n Il est possible de changer le rôle d\u0026rsquo;un noeud via un manager.\nEssayer de passer node2 en tant que manager\n$ docker node promote node2  Combien de manager a-t-on besoin ?\n2N+1 noeuds peuvent tolérer N pannes.\n1 manager = Pas de panne\n3 managers = 1 panne\n5 managers = 2 pannes (Ca nous donne le droit à l\u0026rsquo;erreur en cas de maintenance)\n I ne faut pas mettre trop de manager, en règle générale 5 est suffisant pour un cluster, même très gros.\nPlus on ajoute de managers, plus la réplication Raft mettra de temps.\nLa replication Raft doit essayer de rester sous les 10ms entre les managers.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/install-compose/","title":"Installer docker-compose","tags":[],"description":"","content":"Bien que nous puissions installer Docker Compose à partir des repos officiels Ubuntu, cette version n\u0026rsquo;est pas très à jour.\nNous allons donc installer Docker Compose à partir du repo github de Docker Compose .\nLe site de Docker propose une documentation pour l\u0026#39;installation .\n$ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose $ docker-compose --version "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/namespaces/","title":"Les Namespaces","tags":[],"description":"","content":"Les Namespaces jouent un rôle important dans les conteneurs.\nIls permettent de placer les conteneurs dans leur propre vu du système et limitent ce que l\u0026rsquo;on peut faire et voir.\nIl y a different type de namespaces:\n pid net mnt uts ipc user  Les namespaces font du Kernel et sont actifs dès le démarrage de l\u0026rsquo;OS.\nMême sans l\u0026rsquo;utilisation des conteneurs, il y a au moins un namespace de chaque type qui contient tous les processus du système.\nIls sont donc liés au système et créés grâce à deux Syscall principaux : clone() et unshare()\nLa commande unshare permet de faire appel à ces Syscall.\nnsenter fait apelle au syscall setns() et nous permet d\u0026rsquo;inspecter les namespaces.\nQuand le dernier processus d\u0026rsquo;un namespace s\u0026rsquo;arrête, le namespace est detruit et toutes les ressources qui vont avec.\nOn les retrouve décrit par des fichiers dans /proc/\u0026lt;PID\u0026gt;/ns\nVérifier dans votre VM les namespaces utilisés par init ( PID 1) et dockerd (pidof dockerd)\n Créer son premier namespace net Le netns sera vu dans la partie suivante.\n UTS Nous allons utiliser le namespace uts.\nIl permet concrétement de choisir le hostname du conteneur.\n// Dans la VM $ hostname apinon-droplet-1 $ sudo unshare --uts /bin/bash // Ici on est dans le namespace // hostname my.name.is.bob # hostname my_name_is_bob  Ouvrir un nouveau terminal et vérifier le hostname de la VM.\n On peut quitter le namespace avec ctrl+d ou exit\nmnt On peut aussi isoler les points de montage.\nOuvrir deux terminaux sur la VM.\n// Terminal 1 (dans le namespace) $ sudo unshare --mount /bin/bash $ mount -t tmpfs none /tmp # ls -l /tmp // Terminal 2 (dans la VM) $ ls -l /tmp Nous avons monté un point de montage privé dans notre namespace.\npid Les processus avec un PID namespace voient seulement les processus dans le même PID namespace.\nChaque PID namespace à sa propre arborescence (à partir de 1)\nSi le PID 1 se termine, alors le namespace est terminé (sur Linux, si on kill le PID 1, on a un Kernel Panic)\n// On entre dans notre namespace $ sudo unshare --pid --fork /bin/bash # ps aux  Que se passe t-il ?\n Nous avons créé un PID namespace, mais Linux se base sur le point de montage /proc pour afficher les processus.\nNotre PID namespace à donc encore accès au PID de la VM, même s\u0026rsquo;il ne peut plus intéragir avec.\n// On entre dans notre namespace $ sudo unshare --pid --fork /bin/bash # pidof dockerd # kill -9 $(pidof dockerd) bash: kill: (7807) - No such process Pour contourner cela, la command unshare fournit l\u0026rsquo;option --mount-proc\n// On entre dans notre namespace $ sudo unshare --pid --fork --mount-proc /bin/bash # ps aux  Il n\u0026rsquo;est pas obligatoire de comprendre tout ce qui suit, mais une explication s\u0026rsquo;impose. Vous avez peut être remarqué le \u0026ndash;fork\nC\u0026rsquo;est une complexité due au fonctionnement du syscall unshare() qui exec() le processus en argument.\nSi on ne fork pas le processus qui lance le syscall, celui-ci va lancer le namespace puis se terminer dans l\u0026rsquo;OS, ce qui va donc terminer le namespace.\nEn utilisant --fork, unshare va dupliquer le processus après avoir créé le PID namespace. Puis lancer /bin/bash dans le nouveau processus.\n user Le user namespace permet de séparer les UID/GID entre l\u0026rsquo;hôte et le namespace.\nCela permet d\u0026rsquo;être root dans le namespace avec un utilisateur standard de l\u0026rsquo;hôte.\n// Notez que `unshare` est lancé sans `sudo` $ id uid=1000(alexxx) gid=1000(alexxx) groups=1000(alexxx) $ unshare --user /bin/bash # id id=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) La séparation des UID dans docker complique le partage de fichier entre conteneurs.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/image_automation/dockerfile/","title":"Le Dockerfile","tags":[],"description":"","content":"Un Quoi ? Le dockerfile est en gros la recette pour créer une image Docker.\nIl contient toutes les instructions pour indiquer au daemon quoi faire et comment doit être construite notre image.\nLa commande à utiliser est docker build\nNotre premier Dockerfile Vous pouvez utiliser la commande suivante pour nettoyer votre environnement docker du travail précédent.\ndocker rm -f $(docker ps -q); docker system prune -af --volumes\nCes commandes suppriment les conteneurs actifs puis les volumes/images/conteneurs inactifs\n FROM et RUN Nous allons construire ensemble le premier Dockerfile.\nLe DockerFile doit être dans un dossier vide.\n$ mkdir mon_image Puis ajouter un Dockerfile dans ce répertoire.\n$ cd mon_image $ touch Dockerfile Lancer l\u0026rsquo;éditeur de votre choix pour modifier le Dockerfile\nFROMubuntuRUN apt-get updateRUN apt-get -y install figlet FROM indique l\u0026rsquo;image de base pour notre build. RUN Execute notre commande pendant le build, RUN ne doit pas être interactif, d\u0026rsquo;où le -y durant le apt-get.  Sauvegarder votre Dockerfile, puis on execute le build\n$ docker build -t figlet .  -t indique le nom de notre image (nous reviendrons sur le nommage juste après) . indique l\u0026rsquo;emplacement du contexte de notre build.  Sending build context to Docker daemon 2.048kB\nle contexte est envoyé à dockerd sous forme de tarball, utile si on build sur une machine distante.\nNous pouvons désormais utiliser notre nouvelle image et executer le programme figlet\n$ docker run -ti figlet root@7d038d8e1960:/# figlet Good Job CMD et ENTRYPOINT Afin de lancer automatiquement un processus dans notre image au lieu de l\u0026rsquo;executer dans un shell, nous pouvons utiliser CMD ou ENTRYPOINT.\n  CMD permet de definir une commande par defaut quand aucune n\u0026rsquo;est donnée au lancement du conteneur.\nUn seul CMD est autorisé, seul le dernier est pris en compte.\n  ENTRYPOINT defini une commande de base.\nLe CMD ou les arguments en ligne de commande seront les paramètres de l\u0026rsquo;ENTRYPOINT\n  Editer le Dockerfile précédent pour ajouter un ENTRYPOINT et un CMD afin d\u0026rsquo;afficher I Love Containers avec figlet.\nla conteneur sera lancé avec docker run ilovecontainers\n Avec cette dernière image, je veux lancer un conteneur en mode interactif sous bash. Quel serait la marche a suivre ? Vous pouvez chercher dans la documentation de Docker pour vous familiariser avec le site:\nDocumentation: Docker build  Tips and Tricks On ne va pas trop rentrer dans les details, mais il faut savoir qu\u0026rsquo;il y a quelques bonnes habitudes à respecter lorsque l\u0026rsquo;on créé un Dockerfile.\nEn voici quelques unes pour ne pas être étonné de les voir, si vous tombez sur certain Dockerfile.\n Il faut reduire le nombre de layers Ne pas installer des paquets inutiles Supprimer les fichiers temporaires et caches avant de changer de layer.\nSachant que chaque layer est indépendant, supprimer un fichier créé dans un layer précedant ne reduira pas la taille de l\u0026rsquo;image.  Pour en savoir plus, le site de Docker est une bonne base d\u0026rsquo;information pour commencer. Je reste dispo pour les questions si besoin.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/networks/network_driver/","title":"Les Pilotes Réseau","tags":[],"description":"","content":"Docker inclut plusieurs drivers Réseau que l\u0026rsquo;on peut choisir avec l\u0026rsquo;option --net \u0026lt;driver\u0026gt;\n bridge (par defaut) none host container  Le bridge Par defaut le conteneur obtient une interface virtuelle eth0 en plus de son interface de bouclage (127.0.0.1)\nCette interface est fournie par une paire de veth.\nElle est connectée au Bridge Docker appelé docker0 par defaut.\nLes adresses sont allouées dans un réseau privé interne 172.17.0.0/16 par défault.\nLe trafic sortant passe par une régle iptables MASQUERADE, puis le trafic entrant et naté par DNAT.\nLes régles sont automatiquement gérées par Docker.\nLe null driver Pas grand chose à dire sur celui-là, Si ce n\u0026rsquo;est que le conteneur ne peut pas envoyer ni recevoir de trafic.\nIl obtient uniquement son adresse local lo\nLe host driver Le conteneur executé avec ce driver voit et accède aux interfaces de l\u0026rsquo;hôte.\nLe trafic n\u0026rsquo;est donc pas naté et ne passe pas par une veth.\nCe driver permet donc d\u0026rsquo;avoir les performances natives de la carte réseau. Très pratique dans des applications sensibles à la latence (Voip, streaming, \u0026hellip;)\nLe driver container Celui-ci est un peu spécial car il permet de réutiliser la stack réseau d\u0026rsquo;un autre conteneur.\nLes deux conteneurs partagent la même interface, IP, routes, \u0026hellip;\nIls peuvent communiquer au travers de 127.0.0.1.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/work_with_images/image_vs_container/","title":"Image vs Conteneur","tags":[],"description":"","content":"Une image n\u0026rsquo;est pas un conteneur ! Une image est un système de fichiers en lecture seule\nUn conteneur est processus qui s\u0026rsquo;execute dans une copie de ce système de fichiers.\nPour accélérer le démarrage et optimiser les accès disque, plutôt que de copier l\u0026rsquo;image entière, on utilise ici du Copy-On-Write.\nPlusieurs conteneurs peuvent donc utiliser la même image sans dupliquer les données.\nSi une image est en lecture seule, on ne modifie pas une image, on en crée une nouvelle.\nNous avons utilisé l\u0026rsquo;image ubuntu tout a l\u0026rsquo;heure.\nNous pouvons inspecter ses layers de la manière suivante\n$ docker image history ubuntu:latest IMAGE CREATED CREATED BY SIZE COMMENT 1d622ef86b13 7 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 7 weeks ago /bin/sh -c mkdir -p /run/systemd \u0026amp;\u0026amp; echo \u0026#39;do… 7B \u0026lt;missing\u0026gt; 7 weeks ago /bin/sh -c set -xe \u0026amp;\u0026amp; echo \u0026#39;#!/bin/sh\u0026#39; \u0026gt; /… 811B \u0026lt;missing\u0026gt; 7 weeks ago /bin/sh -c [ -z \u0026#34;$(apt-get indextargets)\u0026#34; ] 1.01MB \u0026lt;missing\u0026gt; 7 weeks ago /bin/sh -c #(nop) ADD file:a58c8b447951f9e30… 72.8MB Les données relatives aux images et aux conteneurs sont stockées dans: /var/lib/docker/\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/work_with_container/","title":"Travailler avec les conteneurs","tags":[],"description":"","content":"Hello World Dans votre nouvel environnement, tapez la commande suivante:\n$ docker run busybox echo hello world hello world  Nous avons utilisé une des plus simple et petite image: busybox busybox est souvent utilisé dans les systèmes embarqués. Nous avons lancé un simple processus et affiché hello world La premiere fois que l\u0026rsquo;on lance un conteneur, l\u0026rsquo;image est chargée sur la machine, cela explique les lignes supplémentaires.  Conteneur interactif Lançons un conteneur un peu plus sympa\n$ docker run -it ubuntu root@ae1c076701b7:/#   Nous venons de lancer un simple conteneur sous ubuntu\n  -it est un raccourci pour -i -t.\n  -i nous connecte au stdin du conteneur\n  -t nous donne un pseudo-terminal dans le conteneur\n    Utiliser le conteneur Essayez de lancer figlet dans notre conteneur\nroot@ae1c076701b7:/# figlet bash: figlet: command not found Nous avons besoin de l\u0026rsquo;installer\nroot@ae1c076701b7:/# apt-get update \u0026amp;\u0026amp; apt-get install figlet -y [...] root@ae1c076701b7:/# figlet hello-world _ _ _ _ _ | |__ ___| | | ___ __ _____ _ __| | __| | | \u0026#39;_ \\ / _ \\ | |/ _ \\ ____\\ \\ /\\ / / _ \\| \u0026#39;__| |/ _` | | | | | __/ | | (_) |_____\\ V V / (_) | | | | (_| | |_| |_|\\___|_|_|\\___/ \\_/\\_/ \\___/|_| |_|\\__,_| Conteneurs et VMs Sortir du conteneur avec exit et lancer la commande à nouveau figlet hello-world, cela fonctionne-t-il ?\n Nous avons lancé un conteneur ubuntusur une machine hôte linux. Ils ont des paquets differents et sont independants, même si l\u0026rsquo;OS est identique..  Mais où est mon conteneur ? Notre conteneur à maintenant un statut `stopped. Il est toujours présent sur le disque de la machine mais tous les processus sont arrétés.\nNous pouvons lancer un nouveau conteneur, et lancer figlet à nouveau\nroot@b6cb64d4bddc:/# figlet bash: figlet: command not found Nous avons lancé un tout nouveau conteneur avec la même image de base ubuntu et figlet n\u0026rsquo;est pas installé.\nIl est possible réutiliser un conteneur qui à été arrêté mais ce n\u0026rsquo;est pas la philosophie des conteneurs.\nVoyez un conteneur comme un processus à usage unique, si l\u0026rsquo;on veut réutiliser un conteneur personnalisé, on crée une image\nCela permet de garder le coté immuable d\u0026rsquo;un conteneur et de pouvoir le partager de facon fiable.\nNous verrons dans un prochain chapitre comment personnaliser une image !\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/","title":"Pourquoi docker ?","tags":[],"description":"","content":"Avant:\n application en un seul bloc Cycle de developpement long Un seul environnement de prod Scalabilitée lente  Aujourd\u0026rsquo;hui:\n Architecture orientée microservices Mise a jour fréquente et rapide Environnement multiple Besoin de scalabilité rapide  "},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/","title":"Introduction","tags":[],"description":"","content":"Partie 1 Introduction Introduction à docker et au concept de conteneurs.\nOn ne lancera pas (tout de suite) de conteneur dans ce chapitre !\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/swarm/create_service/","title":"Créer un service","tags":[],"description":"","content":" Pour les besoins du TP, on va garder un manager et 2 workers.\nLes applications ne sont pas critiques et nous sommes en mode POC.\nOn va donc ne garder que node1 en tant que manager.\nExecuter la commande suivante sur node1 :\n$ docker node demote node2\n$ docker node demote node3\n  Lancer le service On lance un service avec la commande docker service create ..., on peut faire l\u0026rsquo;analogie avec docker run ...\nCréer un service avec une image alpine qui va ping 1.1.1.1:\n$ docker service create --name pingpong alpine ping 1.1.1.1 Vérifier le resultat\n$ docker service ps pingpong Inspecter les logs De la même manière que l\u0026rsquo;on irait voir les journaux d\u0026rsquo;un conteneur avec docker logs ...\nOn utilise ici docker service logs ... \n$ docker service logs pingpong Avec la commande docker service ps on peut voir où notre task a été deployé.\n$ docker service ps pingpong // Chercher dans la colonne NODE Connectez vous sur ce noeud et lister les conteneur docker ps puis verifier les logs de notre conteneur.\nRevenez ensuite sur le manager.\nScaling On va maintenant créer 2 copies de notre service sur chaque noeud du cluster.\n$ docker service scale pingpong=6 Vérifier avec docker service ps ... où sont deployés les tasks, et vérifier avec docker ps les conteneurs sur node1\nOn voit que les opérations de scaling peuvent prendre du temps. Si l\u0026rsquo;on souhaite recupérer la main, on peut utiliser --detach=true.\nLa commande ne va pas se terminer plus rapidement, mais on pourra éxécuter d\u0026rsquo;autres commandes pendant ce temps.\nVoyons ca de suite:\n$ docker service scale pingpong=24 --detach=true \u0026amp;\u0026amp; watch -n1 \u0026#39;docker service ps pingpong\u0026#39; On peut maintenant arrêter le flood :)\n$ docker service rm pingpong Avec un port On peut exposer un service de la même manière qu\u0026rsquo;avec la commande docker run, à quelques différances près. Avec docker service create -p HOST:TASK :\n Le port HOST sera disponible sur TOUS les noeuds du swarm. Les requêtes seront Load Balancé entre toutes les tasks.  On va deployer un service ElasticSearch, on va l\u0026rsquo;appeler demo\n$ docker service create --name demo --publish 9200:9200 --replicas 5 Pendant l\u0026rsquo;initialisation du service, on peut voir plusieurs etapes.\n assigned (assignement de la task à un noeud) preparing (téléchargement de l\u0026rsquo;image) starting running  Quand une task est arrêtée, elle ne peut pas être redémarrée, une nouvelle sera créée à sa place.\nOn peut tester notre service, il écoute sur le port 9200 des noeuds du cluster.\n$ curl -sL 127.0.0.1:9200 ElasticSearch nous retourne un json avec les infos sur l\u0026rsquo;instance.\nOn retrouve des noms de Super Heros dans la clef name.\nEssayons d\u0026rsquo;executer la commande plusieurs fois, on devrait voir plusieurs noms.\nfor N in $(seq 1 10); do curl -sL 127.0.0.1:9200 | jq -r \u0026#39;.name\u0026#39; done Le trafic est géré par le routing mesh.\nChaque requête est delivrée par une des instances de notre service en Round Robin.\nLe LoadBalancing est opérée par IPVS, chaque noeud à donc son LoadBalancer.\nMais IPVS ne gére pas le Layer7.\nIl faut un service dans Swarm pour router les requêtes vers les bons services.\nCe service soit être compatible avec Swarm pour modifier sa config dynamiquement.\nIl existe par exemple Traefik qui le fait très bien.\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/compose-app/","title":"Une app Docker Compose","tags":[],"description":"","content":"Docker fournit un repository Github avec plusieurs applications pour tester Docker.\nNous allons utiliser l\u0026rsquo;application dockercoins pour essayer Docker Compose.\nJ\u0026rsquo;ai préparé l\u0026rsquo;application dans le repo du TP.\nVous pouvez récupérer le repo sur la machine en clonant le repo git du TP.\n$ git clone https://github.com/zaggash/tp-iut-docker.git $ cd dockercoins/ On ne va pas rentrer dans trop de details concernant la syntaxe d\u0026rsquo;un docker-compose.yaml.\nCela prendrait beaucoup de temps et la documentation de Docker est un bien meilleur référentiel avec un tas d\u0026rsquo;exemples et d\u0026rsquo;explications.\nLancer l\u0026rsquo;application Docker compose est très pratique en mode developpement.\nCela permet de lancer une stack applicative complète à partir des fichiers présents sur notre machine.\nOn lance l\u0026rsquo;application avec docker-compose\n$ docker-compose up Compose demande à Docker de construire l\u0026rsquo;application en créant des images, puis de lancer les conteneurs et enfin afficher les logs.\nL\u0026rsquo;application est composé de 5 services:\n* rng = un service web qui génére des bits aléatoires\n* hasher = un service web qui hash les bits reçu\n* worker = Un processus qui qui fait appel a rng et haser\n* webui = Une interface web\n* redis = La base de donnée\nworker fait appel à rng avec un GET pour générer un byte aléatoire, puis il le renvoie avec un POST à hasher.\nIndéfiniment\u0026hellip;\nworker met à jour redis a chaque boucle.\nwebui permet d\u0026rsquo;afficher les resultats.\nAucune adresse IP n\u0026rsquo;est spécifiée, que ce soit dans le code ou dans le docker-compose.yaml.\nLes applications utilisent le nom des services respectifs pour atteindre les conteneurs. Vous pouvez voir dans worker/worker.py\nredis = Redis(\u0026#34;redis\u0026#34;) def get_random_bytes(): r = requests.get(\u0026#34;http://rng/32\u0026#34;) return r.content def hash_bytes(data): r = requests.post(\u0026#34;http://hasher/\u0026#34;, data=data, headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/octet-stream\u0026#34;})  Chercher avec les commandes docker ou dans le fichier compose, le port sur lequel est exposé webui\n On peut maintenant arrêter l\u0026rsquo;application avec un ctrl+c\nDocker va stopper les conteneur avec un TERM puis un KILL si necessaire.\nOn peut forcer le KILL avec un deuxième ctrl+c.\nEn arrière plan On peut relancer l\u0026rsquo;application en arrière plan\n$ docker-compose up -d // Puis $ docker-compose ps $ docker-compose logs --tail 10 --follow Scaling UP Notre but va être d\u0026rsquo;accélérer l\u0026rsquo;application sans toucher au code. Mais avant on va chercher si il y a un bottleneck ( Pas assez de RAM, de CPU, IO ?)\nEssayer de trouver par vous même, sinon rdv à la suite.\n Pour le CPU et la RAM on va lancer top, puis chercher les cycles idle et la RAM Free.\nvmstat 1 10 va nous donner un extrait sur 10s pour voir les IO disque.\nY a t-il assez de ressource ?\n 2 workers Ajouter un nouveau worker à l\u0026rsquo;application\n$ docker-compose up -d --scale worker=2 Puis ouvrir de nouveau le navigateur sur l\u0026rsquo;interface Web.\nY a t-il un impact sur les ressources ?\n 10 workers alors ! On va donc augmenter les workers jusque 10 et l\u0026rsquo;application ira 10x plus vite.\n$ docker-compose up -d --scale worker=10  Est ce bien le cas ?\n Vérifions les ressources CPU, RAM, IO une nouvelle fois. Puis la latence des deux backend web.\n$ top $ vmstat 1 10 // Latence rng, exposé sur le port 8001 $ httping -fc 10 127.0.0.1:8001 // Latence hasher, sur le port 8002 $ httping -fc 10 127.0.0.1:8002  Quel service pose problème ? Comment peut on résoudre le problème ?\n Stopper l\u0026rsquo;application Avant de passer à autre chose, je suis disponible pour faire un point sur l\u0026rsquo;avancement et les questions que vous pourriez avoir. Des problèmes ?\nOn peut maintenant arrêter l\u0026rsquo;application\n$ docker-compose down "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/networks/inside_network/","title":"Dans le Réseau","tags":[],"description":"","content":"On peut lister les réseaux avec docker network ls\n$ docker network ls NETWORK ID NAME DRIVER SCOPE a5fa804dcca5 bridge bridge local a5a200b4762b host host local b35f65ab844b none null local On peut considérer un réseau comme un Virtual Switch.\nDocker va lui assigner automatiquement un sous-réseau puis une IP aux conteneurs associés.\nLes conteneurs peuvent faire partis de plusieurs réseaux à la fois.\nLes noms des conteneurs sont résolus via un serveur DNS embarqué dans le Docker daemon.\nIl existe aussi un driver multi-hôtes utilisé lors de la mise en cluster de plusieurs machines (un Cluster Swarm)\nCe driver, appelé overlay fonctionne au travers de lien VXLAN. Nous reviendrons sur celui-ci un peu plus tard.\nUn réseau, deux conteneurs, Créer un réseau devops\n$ docker network create devops 8a5841273868138b581a8c663e9a042f181a1a52c0028c21988ffd474c117610 Vous pouvez le voir avec docker network ls\nMaintenant, lancer un conteneur sur ce réseau et donné lui un *nom reconnaissable.\n$ docker run -d --name AppDev --net devops hashicorp/http-echo -text \u0026#34;Mon AppDev\u0026#34; Maintenant, lancer un autre conteneur dans ce même réseau\n$ docker run -ti --net dev alpine sh / # Essayer de lancer un ping vers votre premier conteneur AppDev\n On peut inspecter le réseau avec docker inspect devops\n$ docker inspect devops [ { \u0026#34;Name\u0026#34;: \u0026#34;devops\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;8be916360cbc400758d107e47e02a9890d37da0fe3cb0a3a11acde60173a03de\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2020-06-14T01:49:07.632816857Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.20.0.0/16\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;2496236a9109045a1cfb17cd237e84208fe2b3fee7b861e212aeaf7bce9bf61c\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AppDev\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;19a945be9b13b373dc49c840c9871bfa60ed7bfba163c7a93e763b7f016102dc\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:14:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.20.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] Dans mon cas on peut voir que devops à un sous réseau en 172.20.0.0/16\nPuis la définition de mon conteneur avec l\u0026rsquo;IP 172.20.0.2/16\nMaintenant sur la VM, lister les interfaces réseau\n$ ip a [...] 262: br-8be916360cbc: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:f3:81:04:97 brd ff:ff:ff:ff:ff:ff inet 172.20.0.1/16 brd 172.20.255.255 scope global br-8be916360cbc valid_lft forever preferred_lft forever inet6 fe80::42:f3ff:fe81:497/64 scope link valid_lft forever preferred_lft forever 264: vethcbea7f9@if263: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br-8be916360cbc state UP group default link/ether 96:56:cf:87:0d:3d brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::9456:cfff:fe87:d3d/64 scope link valid_lft forever preferred_lft forever Vous pouvez voir les interfaces de la VM, le bridge docker0.\nPuis deux interfaces br-8be916360cbc, vethcbea7f9@if263 qui sont le bridge et l\u0026rsquo;interface veth coté VM.\nOn peut voir ça aussi avec brctl\n$ brctl show bridge name\tbridge id\tSTP enabled\tinterfaces br-8be916360cbc\t8000.0242f3810497\tno\tveth1eecb09 docker0\t8000.02422beae43c\tno\tOn peut aller un peu plus loin Créér un deuxième réseau prod Lancer un conteneur dans ce réseau prod\n$ docker run -d --name AppProd hashicorp/http-echo -text \u0026#34;Production\u0026#34; Obtenir l\u0026rsquo;IP de AppDev et AppProd\n$ docker run --rm --net container:AppDev alpine ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 265: eth0@if266: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:14:00:02 brd ff:ff:ff:ff:ff:ff inet 172.20.0.2/16 brd 172.20.255.255 scope global eth0 valid_lft forever preferred_lft forever $ docker run --rm --net container:AppProd alpine ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 268: eth0@if269: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:18:00:02 brd ff:ff:ff:ff:ff:ff inet 172.24.0.2/16 brd 172.24.255.255 scope global eth0 valid_lft forever preferred_lft forever On a donc AppDev : 172.20.0.2/16 et AppProd : 172.24.0.2/16\nEssayer de pinger AppProd à partir de AppDev. Que se passe t-il ?\n On a vu que Docker est lié au Kernel, nous allons très simplement connecter AppDev et AppProd sans passer par Docker.\nNous allons créer une paire de Veth, puis connecter un bout au bridge devops et un autre dans le namespace du conteneur AppProd\n# On crée la paire de veth $ sudo ip link add name int_hote type veth peer name int_conteneur # On associe in_hote au bridge $ sudo ip link set int_hote master br-8be916360cbc up # On voit nos interface dans la VM $ ip a | grep int 270: int_conteneur@int_hote: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 271: int_hote@int_conteneur: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue master br-8be916360cbc state LOWERLAYERDOWN group default qlen 1000 Il nous faut trouver le pid de AppProd pour associer l\u0026rsquo;autre veth à son network namespace\n$ ps ax | grep \u0026#39;Production\u0026#39; 3146 ? Ssl 0:00 /http-echo -text Production Le PID est le 3146\n$ ip link set int_conteneur netns 3146 On va utiliser la commande nsenter pour se balader dans les namespaces.\n$ nsenter -n -u -t 3146 root@69421ba25609:~# ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 268: eth0@if269: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:18:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.24.0.2/16 brd 172.24.255.255 scope global eth0 valid_lft forever preferred_lft forever 270: int_conteneur@if271: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 26:d7:1f:f9:fb:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 Maintenant on configure l\u0026rsquo;interface dans le conteneur AppProd.\nroot@69421ba25609:~# ip link set int_conteneur name eth1 up root@69421ba25609:~# ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 268: eth0@if269: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:18:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.24.0.2/16 brd 172.24.255.255 scope global eth0 valid_lft forever preferred_lft forever 270: eth1@if271: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 26:d7:1f:f9:fb:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 # # Rappel, le sous réseau devops est 172.20.0.0/16 # On va prendre un IP dans ce réseau : 172.20.0.100/16 # root@69421ba25609:~# ip addr add 172.20.0.100/16 dev eth1 root@69421ba25609:~# ip route add default via 172.20.0.1  Essayer maintenant de nouveau de pinger AppProd à partir de AppDev.\nConnectez vous au namespace avec nsenter pour jouer avec ;) Que se passe t-il ?\n Supprimer maintenant les deux conteneurs.\nObserver les interfaces réseau au niveau de la VM.\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/image_automation/dockerhub/","title":"Le Docker Hub","tags":[],"description":"","content":"Le Nommage Les images docker doivent respecter un certain schema de nommage pour être partager dans un registry.\nIl y a 3 espaces de noms:\n Les images officielles\nalpine, ubuntu, python  Les images officielles sont selectionées par Docker.\nElles sont directement dans l\u0026rsquo;espace de nom racine.\nCe sont généralement des images de tiers reconnues.\nhttps://hub.docker.com  Les images d\u0026rsquo;utilisateurs (ou d\u0026rsquo;organisations)\nzaggash/random  L\u0026rsquo;espace de nom utilisateur contient les images des utilisateurs ou organisations.\nzaggash est l\u0026rsquo;utilisateur dockerhub.\nrandom est le nom de l\u0026rsquo;image.\n Les images appartenant à un registry autre que le DockerHub\nregistry.mondns.fr:5000/mon_repo/mon_image  Ce nom est composé de l\u0026rsquo;adresse IP ( ou DNS) du registry et du port.\nPuis on retrouve la même logique que précédemment.\nLes tags Vous avez peut être déjà remarqué mais les images ont un tag de version associé à leur nom, le tag par défaut est latest\n$ docker pull zaggash/random Using default tag: latest latest: Pulling from zaggash/random 76df9210b28c: Pull complete Digest: sha256:f1eb69bbb25b4f0b88d2edfe1d5837636c9e5ffaad0e96a11c047005a882f049 Status: Downloaded newer image for zaggash/random:latest docker.io/zaggash/random:latest Le tag defini une variante, la version d\u0026rsquo;une image.\n Si vous n\u0026rsquo;avez pas de compte sur le DockerHub, je vous invite à en créer un, c\u0026rsquo;est gratuit \u0026raquo; Inscription Essayer de pousser votre image créée précédemment, dans votre espace de nom avec docker login et docker push\nVous devrez certainement la renommer avec la commande docker tag \u0026raquo; Documentation Docker CLI   Le Hub Petite Pause !\nOn essaie de tous se retrouver pour une présentation de l\u0026rsquo;interface, et une session Questions/Réponses si besoin.\n  Les images officielles, les images utilisateurs/organisations Les Tags Présentation de l\u0026rsquo;integration avec Github Builds automatisés Les Webhooks  "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/networks/","title":"Les Réseaux","tags":[],"description":"","content":"Le serveur Nginx Pour avoir accès au service web de nginx il va falloir exposer son port. Lancer l\u0026rsquo;image nginx du Dockerhub qui contient un serveur web basique.\n$ docker run -d -p 8080:80 nginx Unable to find image \u0026#39;nginx:latest\u0026#39; locally latest: Pulling from library/nginx 8559a31e96f4: Pull complete 8d69e59170f7: Pull complete 3f9f1ec1d262: Pull complete d1f5ff4f210d: Pull complete 1e22bfa8652e: Pull complete Digest: sha256:21f32f6c08406306d822a0e6e8b7dc81f53f336570e852e25fbe1e3e3d0d0133 Status: Downloaded newer image for nginx:latest 8fe2d550ac86b4bb6f544710f4f65ffcc0f4728a2cf52f5b8455e0112b284ce0 -p \u0026lt;ip\u0026gt;:8080:80 Ici on expose le port 80 du conteneur nginx sur le port 8080 de la VM\nPar defaut, le port de la VM écoute sur 0.0.0.0\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b1fa3fc64a2f nginx \u0026#34;/docker-entrypoint.…\u0026#34; 3 seconds ago Up 2 seconds 0.0.0.0:8080-\u0026gt;80/tcp stupefied_shaw Sur l\u0026rsquo;output ci-dessus, ou voit bien la colonne PORTS qui recapitule les ports ouverts.\nOn lance un curl pour verifier que l\u0026rsquo;on a bien un HTTP 200\n$ curl -sLI 127.0.0.1:8080 HTTP/1.1 200 OK Server: nginx/1.19.0 Date: Sat, 13 Jun 2020 23:04:53 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 26 May 2020 15:00:20 GMT Connection: keep-alive ETag: \u0026#34;5ecd2f04-264\u0026#34; Accept-Ranges: bytes  Essayer de lancer un autre conteneur nginx qui écoute sur le port 8080 uniquement sur localhost. Que se passe t - il ?\n Supprimer maintenant tous les conteneurs.\nEssayer de lancer un conteneur qui publie le port 80 sur toutes les interfaces et le port 8080 en local.\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/work_with_container/background/","title":"Conteneurs en arrière-plan","tags":[],"description":"","content":"Un conteneur non-interactif Nous allons lancer un conteneur tout simple qui affiche des nombres aléatoires chaque seconde.\n$ docker run zaggash/random 23008 19194 17802 16235 8189 667  Ce conteneur continuera de s\u0026rsquo;executer indéfiniement. Un ctrl+c permet de l\u0026rsquo;arrêter.  en arrière-plan Nous pouvons lancer ce conteneur de la meme manière mais en arrière plan avec l\u0026rsquo;option -d\n$ docker run -d zaggash/random a5a20f1f8897d6b7a7644a322141ad74a3c21e28530b11cf10ef583ba539e55c On ne voit plus la sortie standard du conteneur, mais le daemon dockerd collecte toujours stdin/stdout du conteneur et les écrit dans un fichier de log.\nLa chaîne de caractères est l\u0026rsquo;ID complet de notre conteneur.\nPetit rappel du chapître précedent.\nOn peut voir le processus dockerd, PID 7807 qui contient la socket de containerd en argument.\nPuis notre processus enfant 8247, executé par runc, PID 8217 lui même demarré par containerd, PID 2656\n $ ps fxa | grep dockerd -A 3 [...] 2656 ? Ssl 42:56 /usr/bin/containerd 8217 ? Sl 0:00 \\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/027b6c72b74f510b3403a3cd246e3c8c802034960cb82bf45dad8278f0e21d6c -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc 8247 ? Ss 0:00 \\_ /bin/sh -c while echo $RANDOM;do sleep 1;done -- 7807 ? Ssl 1:03 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock [...] Un processus conteneurisé est un processus system comme un autre. On peut le voir avec un ps on peut faire l\u0026rsquo;analyser avec un strace, lsof,\u0026hellip;\nPlus de commandes Verifier l\u0026rsquo;état de notre conteneur Verifier les conteneurs en cours d\u0026rsquo;execution avec la commande docker ps\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a5a20f1f8897 zaggash/random \u0026#34;/bin/sh -c \u0026#39;while e…\u0026#34; 5 minutes ago Up 5 minutes crazy_khorana L\u0026rsquo;API nous retourne:\n l\u0026rsquo;ID tronqué de notre conteneur l\u0026rsquo;image utilisé par le conteneur l\u0026rsquo;etat du conteneur (Up) un nom généré aléatoirement  Lancer 2/3 conteneurs supplémentaires et verifier que docker ps nous retourne tous les conteneurs.\n Quelques commandes utiles 1 - Voir les ID des conteneurs Si vous voulez lister seulement les IDs des conteneurs, l\u0026rsquo;option -q renvoi une colonne sans les entêtes. Cet argument est particulièrement utile pour le scripting.\ndocker ps -q eaf444d185be aaeb4643ae39 a5a20f1f8897 2 - Voir les logs des conteneurs Docker garde les logs stderr et stdout de chaque conteneur. Verifions ça avec notre premier conteneur\n$ docker logs a5a [...] 5412 3585 13237 20376 29438 Docker nous retourne la totalité des logs du conteneur.\nPour eviter d\u0026rsquo;être polluer par tout ça, nous pouvons utiliser l\u0026rsquo;argument --tail et extraire les dernieres lignes\n$ docker logs --tail 5 a5a 12893 32068 25356 571 16054 Pour voir les logs en temps réel, on peut utiliser l\u0026rsquo;argument -f\n$ docker logs --tail 5 -f a5a 6644 28412 3315 22610 27692 3136 9107 20481 ^C Nous voyons les 5 dernières lignes de logs puis l\u0026rsquo;affichage en temps réél. ctrl+c pour quitter.\n3 - Arrêter un conteneur Nous pouvons arrêter un conteneur de deux manières.\n avec un kill avec un stop  Le kill va arrêter le conteneur de manière immediate avec un signal KILL.\nLe stop envoie un signal TERM et peut être intercepté par l\u0026rsquo;application pour terminer le processus.\nAprès 10s si le processus n\u0026rsquo;est pas arrêté, Docker envoi un KILL\nOn peut tester ça avec notre conteneur\n$ docker stop a5a a5a Nous voyons bien que le terminal nous rend la main après une dizaine de seconde.\n Docker envoi un TERM Le conteneur ne réagit pas à ce signel, c\u0026rsquo;est une simple boucle en Shell. 10s après, le conteneur est toujours actif, alors Docker envoi un KILL et termine le conteneur.  Maintenant, on va arrêter les conteneurs restant avec un kill en utilisant les commandes vues précédemment\n$ docker kill $(docker ps -q) eaf444d185be aaeb4643ae39 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Nous voyons ici que les conteneurs ont été arrêtés immediatement.\n$ docker ps -a docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eaf444d185be zaggash/random \u0026#34;/bin/sh -c \u0026#39;while e…\u0026#34; 29 minutes ago Exited (137) 3 minutes ago friendly_chatelet aaeb4643ae39 zaggash/random \u0026#34;/bin/sh -c \u0026#39;while e…\u0026#34; 29 minutes ago Exited (137) 3 minutes ago recursing_dirac a5a20f1f8897 zaggash/random \u0026#34;/bin/sh -c \u0026#39;while e…\u0026#34; 44 minutes ago Exited (137) 8 minutes ago crazy_khorana "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/work_with_images/interactive_image/","title":"Création Image Interactive","tags":[],"description":"","content":"Créer une image à partir d\u0026rsquo;un conteneur Il est possible de créer une image partir d\u0026rsquo;un conteneur et de ses modifications.\nMeme si cette solution n\u0026rsquo;est pas la plus utilisée, elle peut être utilsée à des fins de tests ou de sauvegarde.\nReprenons notre exemple avec figlet pour créer une nouvelle image à partir du conteneur. Pour cela nous allons:\n Lancer un conteneur avec une image de base de votre choix. Installer un programme manuellement dans le conteneur Puis utiliser les nouvelles commandes : docker commit, docker tag et docker diff docker diff pour voir les changements effectués dans le conteneur. docker commit pour convertir le conteneur en nouvelle image docker tag pour renommer l\u0026rsquo;image.  Essayez par vous même, sinon je reste disponible pour toutes questions.\n Dans le prochain chapitre, nous allons apprendre à automatiser le build avec un Dockerfile\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/work_with_images/","title":"Travailler avec les images","tags":[],"description":"","content":"Qu\u0026rsquo;est ce qu\u0026rsquo;une image ? Une image est un ensemble de fichiers et de metadata.\n Les fichiers constituent le FileSystem de notre conteneur. les metadata peuvent être de différentes formes  le créateur de l\u0026rsquo;image les variables d\u0026rsquo;environnement les commandes à executer    Les images sont en fait une superposition de couches appelées layers\nChaque layer ajoute, modifie ou supprime un fichier et/ou une metadata.\nLes images peuvent partager des layers, ce qui permet d\u0026rsquo;optimiser l\u0026rsquo;utilisation de l\u0026rsquo;espace disque, les transferts réseaux\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/","title":"Docker","tags":[],"description":"","content":"Partie 2 Docker Dans ce chapitre nous allons voir plusieurs types de conteneur.\nPuis en apprendre un peu plus sur leurs fonctionnements.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/architecture/internal_architecture/","title":"Docker Engine","tags":[],"description":"","content":"Le Docker Engine est divisé en plusieurs parties.\n dockerd (REST API, authentification, réseaux, stockage) : Fait appel à containerd containerd (Gère la vie des conteneurs, push/pull les images) runc (Lance l\u0026rsquo;application du conteneur) containerd-shim (Par conteneur; permet de separer le processus et RunC)  Plusieurs fonctionnalitées sont progressivement deleguées du Docker Engine à containerd\nDes exercices du TP permettrons de verifier cela après l\u0026rsquo;installation\n "},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/architecture/","title":"Architecture","tags":[],"description":"","content":"Lorsque l\u0026rsquo;on installe Docker, on installe plusieurs composants.\nIl y a le Docker Engine et la CLI.\n Le Docker Engine est un demon qui tourne en arrière plan Les interactions avec ce daemon se font via une API REST par un Socket. Sous Linux, ce socket est un socket Unix : /var/run/docker/sock Il est également possible d\u0026rsquo;utiliser un Socket TCP avec authentification TLS. Le Docker CLI communique avec le daemon via cette Socket.  "},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/complex_deployments/","title":"Deploiements complexes","tags":[],"description":"","content":"Les deploiements deviennent de plus en plus compliqués, voici quelques exemples.\nDe nombreuses couches applicatives:\n Language (php, go, JS,\u0026hellip;) Framework Bases de données  Plusieurs environnements cibles:\n Machines locales pour les tests Environnements de Dev, QA, Pre-Prod, Prod Serveurs locaux, Cloud  "},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/swarm/ingress_overlay/","title":"Le réseau Ingress Overlay","tags":[],"description":"","content":" Pour commencer cette partie, nous allons supprimer le service demo\ndocker service rm demo\n Swarm et les Overlay Nous venons de déployer une application sur le cluster et l\u0026rsquo;on a vu que le port du service est disponible sur tous les noeuds.\nCela est possible grâce au réseau par défaut de Swarm, appelé ingress.\n$ docker network ls NETWORK ID NAME DRIVER SCOPE 6b141a0943ca bridge bridge local bfc621566968 docker_gwbridge bridge local 6dbb4bea0e35 host host local 1b6ek4sxqg9g ingress overlay swarm 797221e77f12 none null local C\u0026rsquo;est un réseau overlay installé de base, et nécessaire pour les flux entrant.\nNous pouvons de la même manière que les réseaux bridge qui ont un scope local, créer un réseau overlay qui à un scope au niveau du cluster.\n$ docker network create --driver overlay --subnet 10.10.10.0/24 mon-overlay-demo Puis exécuter un serveur web simple exposé en dehors du cluster sur le port 8080.\nCe service aura 3 réplicas et sera attaché à notre overlay mon-overlay-demo\n$ docker service create --name webapp --replicas=3 --network my-overlay-network -p 8080:80 zaggash/demo-webapp Relever l\u0026rsquo;ID du conteneur qui tourne sur notre noeud actuelle normalement node1\n$ docker ps Puis on entre dans le network namespace du conteneur pour afficher les interfaces.\n$ sudo nsenter -n -t $(pidof -s nginx) # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 59: eth0@if60: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:00:00:16 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.22/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever 61: eth2@if62: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:06 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet 172.18.0.6/16 brd 172.18.255.255 scope global eth2 valid_lft forever preferred_lft forever 63: eth1@if64: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:0a:0a:18 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet 10.10.10.24/24 brd 10.10.10.255 scope global eth1 valid_lft forever preferred_lft forever On voit qu\u0026rsquo;il y en a 3, au lieu de une comme lorsque l\u0026rsquo;on lance un conteneur hors Swarm.\nLe conteneur est connecté à mon mon-overlay-demo au travers de eth1, comme on peux le voir avec l\u0026rsquo;IP.\nLes autres interfaces sont connectées à d\u0026rsquo;autres réseaux. eth0 est le ingress car nous exposons un port vers l\u0026rsquo;extérieur. eth2 est le docker_gwbridge, le bridge local qui permet au conteneur de sortir du cluster.\n$ docker network inspect ingress | grep Subnet \u0026#34;Subnet\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, $ docker network inspect docker_gwbridge | grep Subnet \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, Les Overlays Les réseaux overlay créent un sous réseau qui peut être utilisé par les conteneurs entre plusieurs noeuds dans le cluster.\nLes conteneurs situés sur des noeud différents peuvent échanger des paquets sur ce réseau si ils sont attaché à celui-ci.\nPar exemple, pour notre webapp, on voit qu\u0026rsquo;il y a un conteneur qui tourne sur chaque hôte dans notre cluster. On le vérifie:\n$ docker service ps webapp ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9yrr2kdwc6t5 webapp.1 zaggash/demo-webapp:latest node1 Running Running 37 minutes ago 4qyritdeuwhx webapp.2 zaggash/demo-webapp:latest node3 Running Running 36 minutes ago eiqh6znlcvus webapp.3 zaggash/demo-webapp:latest node2 Running Running 36 minutes ago Se connecter maintenant sur le node2 et essayé de ping le conteneur qui est sur le node1\n(node2) | $ nsenter -n -t $(pidof -s nginx) (node2) | # ping 10.10.10.24 PING 10.10.10.24 (10.10.10.24) 56(84) bytes of data. 64 bytes from 10.10.10.24: icmp_seq=1 ttl=64 time=0.505 ms 64 bytes from 10.10.10.24: icmp_seq=2 ttl=64 time=0.373 ms 64 bytes from 10.10.10.24: icmp_seq=3 ttl=64 time=0.485 ms VXLAN Les réseaux overlay de Docker utilisent la technologie VXLAN qui encapsule les trames ethernet (couche 2 du modèle OSI), dans un datagramme UDP (couche 4).\nCeci permet d’étendre un réseau de couche 2 au dessus de réseaux routés. Les membres de se réseau virtuel peuvent se voir comme s\u0026rsquo;ils étaient connecté sur un switch.\nOn identifie un réseau VXLAN par son identifiant VNI (VXLAN Network Identifier).\nCelle-ci est codée sur 24 bits, ce qui donne 16777216 possibilités, bien plus intéressant que la limite de 4096 induite par les VLANs.\nOn peut le voir en prenant une trace sur les noeuds qui font partis de l\u0026rsquo;overlay.\nRegardons la capture du ping entre le conteneur de node2 vers node1\n(node2) | $ nsenter -n -t $(pidof -s nginx) (node2) | # ping 10.10.10.24 (node1) | $ sudo tcpdump -i ens160 udp and port 4789 [sudo] password for user: tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 19:58:03.259691 IP 10.59.72.7.38266 \u0026gt; node1.4789: VXLAN, flags [I] (0x08), vni 4097 IP 10.10.10.26 \u0026gt; 10.10.10.24: ICMP echo request, id 25411, seq 36, length 64 19:58:03.259756 IP node1.59501 \u0026gt; 10.59.72.7.4789: VXLAN, flags [I] (0x08), vni 4097 IP 10.10.10.24 \u0026gt; 10.10.10.26: ICMP echo reply, id 25411, seq 36, length 64 On peut voir dans les trames ci-dessus, que le premier est le paquet du tunnel VXLAN UDP entre les hôtes dans le port 4789.\nEt à l\u0026rsquo;intérieur, on voit le paquet ICMP entre les conteneurs.\nEncryption Le trafic que l\u0026rsquo;on a vu ci-dessus montre que si l\u0026rsquo;on peut voir les paquets entre les noeuds, on peut voir le trafic entre les conteneurs qui passe dans l\u0026rsquo;overlay.\nC\u0026rsquo;est pourquoi Docker a ajouté une option qui permet de crypter avec IPsec le tunnel VXLAN.\nPour cela, il faut ajouter --opt encrypted lors de la création du réseau.\nRépétons les étapes précédentes en utilisant un overlay crypté.\n(node1) | $ docker service rm webapp (node1) | $ docker network rm mon-overlay-demo (node1) | $ docker network create --driver overlay --opt encrypted --subnet 10.20.20.0/24 mon-overlay-ipsec (node1) | $ docker service create --name webapp --replicas=3 --network mon-overlay-ipsec -p 8080:80 zaggash/demo-webapp (node1) | $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1efc497c5983 zaggash/demo-webapp:latest \u0026#34;/docker-entrypoint.…\u0026#34; 17 seconds ago Up 13 seconds 80/tcp webapp.1.ptfs32hrkcom2nu7syry31bwb (node1) | $ docker inspect 1efc497c5983 | grep IPv4Address \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.26\u0026#34; \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.20.20.3\u0026#34; (node1) | $ sudo nsenter -n -t $(pidof -s nginx) (node1) | # ping 10.20.20.4 PING 10.20.20.4 (10.20.20.4) 56(84) bytes of data. 64 bytes from 10.20.20.4: icmp_seq=1 ttl=64 time=0.503 ms 64 bytes from 10.20.20.4: icmp_seq=2 ttl=64 time=0.412 ms (node1) | $ sudo tcpdump -i ens160 esp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 20:22:42.338614 IP node1 \u0026gt; 10.59.72.7: ESP(spi=0x1916910d,seq=0x19), length 140 20:22:42.338976 IP 10.59.72.7 \u0026gt; node1: ESP(spi=0x01fcdf39,seq=0x19), length 140 20:22:43.349513 IP node1 \u0026gt; 10.59.72.7: ESP(spi=0x1916910d,seq=0x1a), length 140 20:22:43.349913 IP 10.59.72.7 \u0026gt; node1: ESP(spi=0x01fcdf39,seq=0x1a), length 140 Inspecter un réseau Overlay De la même manière que les réseaux bridge, Docker créé une interface bridge pour chaque overlay.\nCe bridge connecte les interfaces virtuelles du tunnel pour établir les connections du tunnel VXLAN entre les hôtes.\nCependant, ces bridges et interfaces de tunnel VXLAN ne sont pas créés directement sur l\u0026rsquo;hôte.\nIls sont dans un conteneur séparé que Docker lance pour chaque réseau overlay.\nPour inspecter ces interfaces, nous devons utiliser nsenter pour accèder à leur namespace.\nDéjà voyons les interfaces de notre conteneur, nous en avons des nouveau depuis le test du tunnel IPsec:\n$ sudo nsenter -n -t $(pidof -s nginx) ifconfig 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 68: eth0@if69: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:00:00:1a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.26/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever 70: eth2@if71: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet 172.18.0.3/16 brd 172.18.255.255 scope global eth2 valid_lft forever preferred_lft forever 72: eth1@if73: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue state UP group default link/ether 02:42:0a:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet 10.20.20.3/24 brd 10.20.20.255 scope global eth1 valid_lft forever preferred_lft forever Voyons le PeerID de la veth eth1 lié à notre overlay.\n$ sudo nsenter -n -t $(pidof -s nginx) ethtool -S eth1 NIC statistics: peer_ifindex: 73 On cherche du coup maintenant la veth avec l\u0026rsquo;index 73, elle doit être dans le namespace de l\u0026rsquo;overlay.\n$ docker network ls | grep mon-overlay-ipsec o2v3e4cf5fro mon-overlay-ipsec overlay swarm $ sudo ls -l /run/docker/netns/ total 0 [...]] -r--r--r-- 1 root root 0 Jun 16 20:16 1-o2v3e4cf5f [...] $ sudo nsenter --net=/run/docker/netns/1-o2v3e4cf5f ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: br0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue state UP group default link/ether be:86:9b:82:98:53 brd ff:ff:ff:ff:ff:ff inet 10.20.20.1/24 brd 10.20.20.255 scope global br0 valid_lft forever preferred_lft forever 65: vxlan0@if65: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue master br0 state UNKNOWN group default link/ether e2:fb:73:9d:d7:a2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 67: veth0@if66: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue master br0 state UP group default link/ether be:86:9b:82:98:53 brd ff:ff:ff:ff:ff:ff link-netnsid 1 73: veth1@if72: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue master br0 state UP group default link/ether ea:86:89:7a:9e:f2 brd ff:ff:ff:ff:ff:ff link-netnsid 2 On voit ici notre interface veth avec l\u0026rsquo;index 73, il s\u0026rsquo;agit de veth1.\nEt nous voyons aussi notre interface VXLAN, vxlan0.\nveth0 est l\u0026rsquo;interface du namespace de la VIP du service.\nOn peut voir l\u0026rsquo;ID de notre VXLAN avec la commande suivante, ici 4098\n$ sudo nsenter --net=/run/docker/netns/1-o2v3e4cf5f ip -d link show vxlan0 65: vxlan0@if65: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1424 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default link/ether e2:fb:73:9d:d7:a2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 1 vxlan id 4098 srcport 0 0 dstport 4789 proxy l2miss l3miss ttl inherit ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx Pour finir, lancer une capture sur l\u0026rsquo;interface virtuelle veth1 va nous montrer le trafic qui quitte le conteneur.\n$ sudo nsenter --net=/run/docker/netns/1-o2v3e4cf5f tcpdump -i veth1 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes 23:02:39.989473 IP 10.20.20.3 \u0026gt; 10.20.20.4: ICMP echo request, id 799, seq 61, length 64 23:02:39.989945 IP 10.20.20.4 \u0026gt; 10.20.20.3: ICMP echo reply, id 799, seq 61, length 64 23:02:41.013488 IP 10.20.20.3 \u0026gt; 10.20.20.4: ICMP echo request, id 799, seq 62, length 64 23:02:41.013979 IP 10.20.20.4 \u0026gt; 10.20.20.3: ICMP echo reply, id 799, seq 62, length 64 23:02:42.037832 IP 10.20.20.3 \u0026gt; 10.20.20.4: ICMP echo request, id 799, seq 63, length 64 23:02:42.038311 IP 10.20.20.4 \u0026gt; 10.20.20.3: ICMP echo reply, id 799, seq 63, length 64 "},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/swarm/","title":"Swarm","tags":[],"description":"","content":"Swarmkit Présentation Swarmkit est un ensemble d\u0026rsquo;outil open-source qui permet de créer un cluster multi-noeuds.\nCet outil fait parti intégrante de Docker.\nC\u0026rsquo;est un système qui fonctionne en mode haute disponibilité basé sur le protocole Raft Ce protocole est robuste et permet une reconfiguration dynamique sans interruption du cluster.\nIl est utilisé dans plusieurs projets open-source comme etcd, zookeeper,\u0026hellip;\nSwarmkit intègre egalement directement le Load-Balancing des services et les réseaux overlay.\nNomenclature   Un cluster Swarm est composé d\u0026rsquo;au moins noeud. Un noeud est soit un manager soit un worker. Les managers s\u0026rsquo;occupent de la partie Raft et conservent les journaux du Raft. Le cluster est commandé via l\u0026rsquo;API Swarmkit au travers des managers. Un seul manager fait office de leader, les autres managers ne font que relayer les requêtes. Les workers recoivent les instructions de la part des managers Il est conseillé d\u0026rsquo;éxécuter le workload sur les workers, bien que les managers peuvent aussi s\u0026rsquo;en charger.   Les managers exposent l\u0026rsquo;API de Swarm On lance des services via l\u0026rsquo;API Un service est un objet defini par sont état désiré : une image, combien de replicas, dans quel réseau,\u0026hellip; Un service est composé de plusieurs tasks Une tasks corresponds à un conteneur assigné à un noeud Les noeuds connaissent leurs tasks et sont chargé de les démarrer ou les arrêter en conséquence.  "},{"uri":"https://zaggash.github.io/tp-iut-docker/image_automation/github/","title":"[Optionel] Github","tags":[],"description":"","content":"Pour aller plus loin\u0026hellip; Si vous le sentez, vous pouvez créer un repo sur Gihub pour pousser votre Dockerfile créé précédemment.\nPuis configurer un build automatique de votre Dockerfile gràace au DockerHub.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/volumes/","title":"Les Volumes","tags":[],"description":"","content":"Les volumes permettent plusieurs choses:\n Passer outre le Copy On Write et utiliser les performances natives des disques. Partager des dossiers et fichiers entre les conteneurs Partager des dossiers et fichiers entre l\u0026rsquo;hôte et les conteneurs Utiliser des points de montage distant  Nous allons voir comment utiliser un volume:\n Dans un Dockerfile Au demarrage avec l\u0026rsquo;option -v En utilisant un volume nommé   La persistance des données Illustrons l\u0026rsquo;état par défaut des données après l\u0026rsquo;arrêt d\u0026rsquo;un conteneur\n$ docker container run -ti --name c1 alpine sh On va créer un dossier et un fichier à l\u0026rsquo;intérieur\n$ mkdir /mon_dossier \u0026amp;\u0026amp; cd /mon_dossier \u0026amp;\u0026amp; touch monfichier.txt Nous allons maintenant voir que le layer R/W du conteneur n\u0026rsquo;est pas accessible depuis l\u0026rsquo;hôte.\nCommençons par quitter le conteneur\n$ exit On va inspecter le notre conteneur pour trouver l\u0026rsquo;emplacement du layer.\nOn peut utiliser la commande inspect et chercher le mot clef GraphDriver\n$ docker container inspect c1 On peut egalement utiliser la sortie avancé grâce au Template Go et voir directement l\u0026rsquo;information.\n$ docker container inspect -f \u0026#39;{{ json .GraphDriver }}\u0026#39; c1 | jq Vous devriez avoir un output qui ressemble à ça:\n{ \u0026quot;Data\u0026quot;: { \u0026quot;LowerDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/0b144df858ed09133fb9de89026b91cb1a8ecacb1464466cff9479f2267b69a0-init/diff:/var/lib/docker/overlay2/3385f7f394c776c48b391cd6e407816d3026cea3ff9f5526f05d631ff6b4ae55/diff\u0026quot;, \u0026quot;MergedDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/0b144df858ed09133fb9de89026b91cb1a8ecacb1464466cff9479f2267b69a0/merged\u0026quot;, \u0026quot;UpperDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/0b144df858ed09133fb9de89026b91cb1a8ecacb1464466cff9479f2267b69a0/diff\u0026quot;, \u0026quot;WorkDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/0b144df858ed09133fb9de89026b91cb1a8ecacb1464466cff9479f2267b69a0/work\u0026quot; }, \u0026quot;Name\u0026quot;: \u0026quot;overlay2\u0026quot; } Depuis l\u0026rsquo;hôte, si on regarde l\u0026rsquo;emplacement du dossier contenu dans la key UpperDir, on peut voir que notre dossier /mon_dossier et notre fichier monfichier.txt sont là.\nExecuter la commande suivante pour voir le contenu de notre dossier /mon_dossier:\n$ ls /var/lib/docker/overlay2/[ID_LAYER]/diff/mon_dossier Que se passe t-il si notre conteneur venait à être supprimé ?\n$ docker container rm c1  Il semble que le dossier UpperDir ci-dessus, n\u0026rsquo;existe plus. Pouvez vous le confirmer ?\nEssayer de lancer de nouveau un ls\n Cela prouve que les données dans un conteneur ne sont pas persistantes, elles sont supprimées en même temps que le conteneur.\nDefinir un volume dans un Dockerfile Nous allons créér un Dockerfile basé sur alpine et definir /mon_dossier en tant que volume.\nCela signifie que tout ce que sera écrit par un conteneur dans /mon_dossier existera en dehors du layer R/W du conteneur.\nUtiliser le Dockerfile suivant:\nFROMalpineVOLUME [\u0026#34;/mon_dossier\u0026#34;]ENTRYPOINT [\u0026#34;/bin/sh\u0026#34;] On definit ici /bin/sh en Entrypoint afin d\u0026rsquo;avoir un shell en mode intéractif sans devoir spécifier de commande.\n Nous pouvons alors lancer la création de l\u0026rsquo;image\n$ docker image build -t img1 . On peut alors lancer notre conteneur en mode intéractif à partir de cette image.\n$ docker container run -ti --name c2 img1 /# On peut donc maintenant aller dans /mon_dossier et créer un fichier.\n/# cd /mon_dossier /# touch hello.txt /# ls hello.txt On peut quitter le conteneur en le laissant tourner en arrière plan.\nIl faut utiliser la combinaison de raccourcis : ctrl+P/ctrl+Q\nPuis vérifier que le conteneur est toujours actif.\n$ docker ps  Le conteneur c2 devrait être listé\n On va alors inspecter ce conteneur pour connaître l\u0026rsquo;emplacement de notre volume sur la VM.\nOn va utiliser directement les templates GO mais on pourrait utiliser un docker inspect puis chercher à la main la clef Mount\n$ docker inspect -f \u0026#39;{{ json .Mounts }}\u0026#39; c2 | jq Vous devriez avoir une sortie qui ressemble à ca:\n[ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;2d47da3c88436afa4b35b084ba0060009066b26b042ee7b161b1d3215f1b06fd\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/2d47da3c88436afa4b35b084ba0060009066b26b042ee7b161b1d3215f1b06fd/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/mon_dossier\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ] Cette sortie montre que le volume /mon_dossier est situé dans /var/lib/docker/volumes/[ID_VOLUME]/_data\nRemplacer le chemin par le votre et vérifier que le fichier hello.txt est bien présent.\nOn peut maintenant supprimer c2\n$ docker rm -f c2 Valider que le fichier est toujours disponible à l\u0026rsquo;emplacement précédent.\nDefinir un volume en mode intéractif On a vu comment définir un volume via un Dockerfile, on peut aussi le définir au lancement avec l\u0026rsquo;option -v\nExecutons un conteneur à partir de l\u0026rsquo;image alpine, on utilisera l\u0026rsquo;option -d pour le passer en arrière plan.\nAfin que le processus PID1 du conteneur reste actif, on utilise une commande qui pinger 1.1.1.1 en continue et ecrire le résultat dans /mon_dossier.\n$ docker run -d --name c3 -v /mon_dossier alpine sh -c \u0026#39;ping 1.1.1.1 \u0026gt; /mon_dossier/ping.txt\u0026#39; Allons chercher l\u0026rsquo;emplacement du volume:\n$ docker inspect -f \u0026#39;{{ json .Mounts }}\u0026#39; c3 | jq Nous avons quasiment la même sortie qu\u0026rsquo;avec le Dockefile, à l\u0026rsquo;exception des IDs\n[ { \u0026quot;Type\u0026quot;: \u0026quot;volume\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;0534a1308b43b5bb7f4f728daeedea7e9962a65b47df4d37e01b2ef89510bd13\u0026quot;, \u0026quot;Source\u0026quot;: \u0026quot;/var/lib/docker/volumes/0534a1308b43b5bb7f4f728daeedea7e9962a65b47df4d37e01b2ef89510bd13/_data\u0026quot;, \u0026quot;Destination\u0026quot;: \u0026quot;/mon_dossier\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Mode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;RW\u0026quot;: true, \u0026quot;Propagation\u0026quot;: \u0026quot;\u0026quot; } ] Vérifions que le fichier existe bien dans le volume:\n$ tail -f /var/lib/docker/volumes/\u0026lt;VOLUME_ID\u0026gt;/_data/ping.txt 64 bytes from 1.1.1.1: seq=11 ttl=59 time=0.807 ms 64 bytes from 1.1.1.1: seq=12 ttl=59 time=0.875 ms 64 bytes from 1.1.1.1: seq=13 ttl=59 time=0.828 ms 64 bytes from 1.1.1.1: seq=14 ttl=59 time=1.101 ms 64 bytes from 1.1.1.1: seq=15 ttl=59 time=1.039 ms 64 bytes from 1.1.1.1: seq=16 ttl=59 time=0.804 ms [...] Le fichier ping.txt est rempli regulièrement par la commande de notre conteneur.\nSi on supprime le conteneur, le processus va arrêter de remplir le fichier mais il ne sera psa supprimé.\nUtiliser un volume nommé Nous allons utiliser la commande pour créer un volume nommé web.\n$ docker volume create --name web Si nous listons les volumes existant, il devrait y avoir notre volume web.\n$ docker volume ls L\u0026rsquo;output devrait ressembler à ça:\nDRIVER VOLUME NAME [...]] local web Pour les volumes, comme presque tous les objets dans Docker, on peut executer la commande inpect.\n$ docker volume inspect web [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2020-06-14T14:27:06Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/web/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Le Mountpoint défini ici est le chemin sur l\u0026rsquo;hôte ou l\u0026rsquo;on peut trouver le volume.\nOn peut voir que le chemin des volumes nommés utilise le nom du volume au lieu de l\u0026rsquo;ID comme dans les exemples précédents.\nOn peut maintenant utiliser ce volume et le monter dans un conteneur.\nNous allons utiliser nginx et monter le volume web dans le répertoire /usr/share/nginx/html du conteneur.\n/usr/share/nginx/html est le répertoire par defaut du serveur nginx. Il contient 2 fichiers : index.html et 50x.html\n $ docker run -d --name www -p 8080:80 -v web:/usr/share/nginx/html nginx Depuis l\u0026rsquo;hôte, allons voir le contenu du volume.\n$ ls /var/lib/docker/volumes/web/_data 50x.html index.html Le contenu du dossier /usr/share/nginx/html du conteneur www à été copié dans le dossier /var/lib/docker/volumes/html/_data sur l\u0026rsquo;hôte.\nAllons vérifier la page d\u0026rsquo;accueil de nginx\n$ curl 127.0.0.1:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Depuis l\u0026rsquo;hôte, nous pouvons désormais modifier le fichier index.html et vérifier que nos changements sont bien pris en compte par le conteneur.\n$ cat\u0026lt;\u0026lt;END \u0026gt;/var/lib/docker/volumes/web/_data/index.html TOC TOC TOC ! END Allons voir de nouveau la page web\n$ curl 127.0.0.1:8080 TOC TOC TOC ! Nous pouvons voir les changements que nous avons effectués.\nMonter un dossier de la VM dans un conteneur. Nous allons maintenant monter un dossier de l\u0026rsquo;hôte dans un conteneur en faisant un bind-mount avec l\u0026rsquo;option -v : -v CHEMIN_HOTE:CHEMIN_CONTENEUR\nCHEMIN_HOTE et CHEMIN_CONTENEUR peuvent être un dossier ou un fichier.\nLe chemin sur l\u0026rsquo;hôte doit exister.\n Il y a deux cas bien distincts:\n le CHEMIN_CONTENEUR n\u0026rsquo;existe pas dans le conteneur. le CHEMIN_CONTENEUR existe dans le conteneur.  N\u0026rsquo;existe pas Executer un conteneur alpine en montant le /tmp local dans le dossier /mon_dossier du conteneur.\n$ docker run -ti -v /tmp:/mon_dossier alpine sh On arrive dans le shell de notre conteneur. Par défaut, il n\u0026rsquo;a pas de répertoire /mon_dossier dans l\u0026rsquo;image alpine.\nQuel est l\u0026rsquo;impact de notre bind mount ?\n$ ls /mon_dossier Le répertoire /mon_dossier à été créé dans le conteneur et contient les fichiers de /tmp de la VM.\nNous pouvons maintenant modifier ces fichiers à partir du conteneur ou de l\u0026rsquo;hôte.\nExiste Executer un conteneur nginx en montant le /tmp local dans le dossier /usr/share/nginx/html du conteneur.\n$ docker run -ti -v /tmp:/usr/share/nginx/html nginx bash Est ce que les fichiers par défaut index.html et 50x.html sont présents dans le dossier /usr/share/nginx/html du conteneur ?\n$ ls /usr/share/nginx/html Non.\nLe contenu du dossier du conteneur à été remplacé avec le contenu du dossier de l\u0026rsquo;hôte.\nLes bind-mount sont utiles en mode développement car ils permettent, par exemple, de partager le code source de l\u0026rsquo;hôte avec le conteneur.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/image_automation/","title":"Automatisation d&#39;une image","tags":[],"description":"","content":"Chapter 3 L\u0026rsquo;automatisation de la création d\u0026rsquo;une image Dans ce chapître, nous allons utiliser les Dockerfile, puis faire un tour d\u0026rsquo;horizon du DockerHub.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/docker/copy_on_write/","title":"Copy-On-Write","tags":[],"description":"","content":"Le copy-on-write Le copy-on-write ( CoW ) permet de partager les layers des images entre les conteneurs.\nDès que le conteneur à besoin d\u0026rsquo;ecrire dans un fichier existant dans une image, celui-ci est copié dans le layer en écriture du conteneur puis modifié.\nOn retrouve ce principe dans les snapshots BTRFS, le provisioning VMwawre,\u0026hellip;\nGrâce à cela, le demarrage des conteneurs est rapide, pas besoin de copier l\u0026rsquo;image.\nLe système de fichier CoW recommandé et supporté par docker est Overlay2\nL\u0026rsquo;avantage est qu\u0026rsquo;il est disponible sur tous les kernel linux recents.\nLes conteneurs qui ecrivent beaucoup de données vont par contre consommer plus d\u0026rsquo;espace et seront plus lents, d\u0026rsquo;autant plus si le fichier à copier est gros. Dans ce cas la, l\u0026rsquo;utilisation d\u0026rsquo;un volume est recommandé.\nLes volumes seront abordés un peu plus tard dans les chapitres.\n Démo  Lancer 5 conteneurs avec l\u0026rsquo;image utilisée précédemment zaggash/random Chercher les IDs des conteneurs fraîchement démarrés.   Lancer un shell dans un conteneur puis ajouter un fichier dans /root Inspecter le conteneur avec docker inspect puis chercher dans le json:  [...] \u0026#34;GraphDriver\u0026#34;: { \u0026#34;Data\u0026#34;: { [...] \u0026#34;MergedDir\u0026#34;: \u0026#34;[...]\u0026#34;, ...  Verifier le contenu de ce dossier. Executer la commande mount sur la VM, qu\u0026rsquo;en concluez vous ?  "},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/install_docker/","title":"Installer Docker","tags":[],"description":"","content":"Dans cette partie, nous allons prendre la main sur les VMs et installer Docker qui nous servira tout au long de la suite du TP.\nConnection à la VM Dans un premier temps, se connecter en SSH à la VM.\nAfin de préparer l\u0026rsquo;environnement pour la suite, l\u0026rsquo;installation devra se faire sur les 3 VMs.\nssh [-i private_key] user@hostname Mettre à jour l\u0026rsquo;OS Afin d\u0026rsquo;être dans les meilleurs conditions possible et que nos machines soient identiques, commençons par mettre à jour les VMs.\n$ sudo apt-get update $ sudo apt-get upgrade -y $ sync \u0026amp;\u0026amp; sync \u0026amp;\u0026amp; sudo reboot Puis procéder à l\u0026rsquo;installation des paquets qui nous serviront par la suite.\n$ sudo apt-get install -y bridge-utils jq git httping Installer Docker La procédure d\u0026rsquo;installation est bien detaillée sur le site de Docker.\nInstaller Docker sur Ubuntu TLDR:\n$ sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo apt-key fingerprint 0EBFCD88 $ sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io Verifier que le daemon est bien lancé\n$ sudo systemctl status docker docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2020-06-11 20:29:54 UTC; 9min ago Docs: https://docs.docker.com Main PID: 7807 (dockerd) Tasks: 30 [...] Lancer un premier conteneur L\u0026rsquo;accès à docker est considéré comme un accès root sur le serveur.\nC\u0026rsquo;est pourquoi l\u0026rsquo;utilisateur \u0026ldquo;docker\u0026rdquo; est équivalent à \u0026ldquo;root\u0026rdquo;.\n Pour simplifier l\u0026rsquo;execution des commandes et éviter de taper \u0026ldquo;sudo\u0026rdquo; devant chaque commande, nous allons ajouter notre utilisateur au groupe \u0026ldquo;docker\u0026rdquo;.\n$ sudo usermod -a -G docker user [ relancer la connection ssh ]\n$ docker run --rm hello-world Hello from Docker! [...] Voilà vous avez executé un premier conteneur\u0026hellip;\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/architecture/namespaces_cgroups/","title":"Namespaces/Cgroups/CoW","tags":[],"description":"","content":"Docker est extrémement lié au Kernel.\nLe fonctionnement des conteneurs repose sur les namespaces, les cgroups et le CopyOnWrite.\nMais egalement d\u0026rsquo;autres aspects liés à la sécurité comme les CAPabilities, seccomp,\u0026hellip;\nCeux qui nous intéressent aujourd\u0026rsquo;hui sont les trois premiers : namespaces, cgroups et le CopyOnWrite.\nCes aspects seront abordés au cours des exercices du TP.\n Brièvement, les namespaces permettent l\u0026rsquo;isolation des processus à différent niveaux (PID, User, Network, Mount)\nLes Cgroups permettent l\u0026rsquo;isolation, la limitation de l\u0026rsquo;utilisation des ressources (Processeur, Memoire, Utilisation Disque)\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/easier_deployments/","title":"Deploiements simplifiés","tags":[],"description":"","content":"Une application construite dans une image peut tourner n\u0026rsquo;importe où.\nCela simplifie la chaine de deploiement et assure que l\u0026rsquo;application soit la même partout où elle s\u0026rsquo;execute.\nUn conteneur retire les problématiques de dépendances, de différences de paquets entre les OS, de configurations qui différent.\nOn oublie le fameux: \u0026ldquo;Moi, ca marche sur ma machine\u0026hellip;\u0026rdquo;\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/swarm/multi_services_app/","title":"Retour sur notre Application","tags":[],"description":"","content":" Pour commencer cette partie, on fait le ménage.\ndocker service rm $(docker service ls -q)\n Nous allons maintenant reprendre l\u0026rsquo;application Dockercoin et la faire tourner dans notre cluster.\nNous allons construire les images, les envoyer sur le hub, puis les exécuter dans le cluster.\nIci nous sommes obligé d\u0026rsquo;envoyer les images dans un registry.\nAvec la commande docker-compose up, toutes les images de nos services sont construite en local.\nMais pour maintenant, nous avons besoin que ces images soit distribuées entre tous les noeuds du cluster.\nLe workflow ressemble un peu ça:\n docker build \u0026hellip; docker push \u0026hellip; docker service create \u0026hellip;  Pour plus de simplicité, nous allons envoyer les images sur le dockerhub.\nMais il serait possible de créer son propre registry ou de les envoyer dans un registry privé.\n Build and Ship Pour se connecter au DockerHub et pouvoir envoyer les images, ne pas oublier de se logguer avec docker login\nPuis ensuite se rendre dans le dossier ou vous avez cloné le dépôt GitHub du TP.\ncd ~/tp-iut-docker/dockercoins/ export REGISTRY=docker.io export USER_NAMESPACE=\u0026lt;votre_dockerhub_id\u0026gt; export TAG=v1.0 for SERVICE in hasher rng webui worker; do docker build -t $REGISTRY/$USER_NAMESPACE/$SERVICE:$TAG ./$SERVICE docker push $REGISTRY/$USER_NAMESPACE/$SERVICE:$TAG done Run Préparer l\u0026rsquo;overlay Rappelez-vous, nos services ont besoin de communiquer entre eux.\nPour cela, nous allons avoir besoin d\u0026rsquo;un réseau overlay pour passer d\u0026rsquo;un noeud à l\u0026rsquo;autre.\n$ docker network create --driver overlay dockercoins $ docker network ls  Bien spécifier \u0026ndash;driver overlay autrement un bridge est créé par défaut.\n Les services On commence par déployer Redis, on utilise ici l\u0026rsquo;image officiel.\n$ docker service create --network dockercoins --name redis redis Puis on démarre les autres services un par un en utilisant les images envoyées just avant.\nexport REGISTRY=docker.io export USER_NAMESPACE=\u0026lt;votre_dockerhub_id\u0026gt; export TAG=v1.0 for SERVICE in hasher rng webui worker; do docker service create --network dockercoins --detach=true \\  --name $SERVICE $REGISTRY/$USER_NAMESPACE/$SERVICE:$TAG done Publier le port de la Webui Nous avons besoin de se connecter à la webui, mais nous n\u0026rsquo;avons publié aucun port.\n$ docker service ps webui $ docker service update webui --publish-add 8000:80 $ docker service ps webui  On peut également supprimer un port publié avec --publish-rm\n On voit que le premier déploiement a été supprimé puis remplacé par la nouvelle version avec le port publié.\nL\u0026rsquo;application est maintenant disponible sur le port 8000.\nVous pouvez ouvrir votre navigateur sur le port 8000 de n\u0026rsquo;importe quel noeud du cluster.\nSale Up Workers On peut également ajouter des worker comme avec docker-compose\n$ docker service update worker --replicas 15 // OU $ docker service scale worker=10 Nous voilà dans la même situation que tout à l\u0026rsquo;heure avec docker-compose mais cette fois au sein d\u0026rsquo;un cluster de 3 machines.\nVous pouvez vérifier sur la webui que la vitesse à bien augmentée.\nrng Nous avions vu que le service rng était le point bloquant.\nIl nous faut maximiser l\u0026rsquo;entropie pour augmenter la génération de hash pour nos worker.\nPour cela, on va lancer une instance du service rng sur chaque noeud.\nSwarmkit à prévu ce genre de déploiement, mais nous devons recréer le service, on ne peux pas activer/désactiver un deploiement globale.\n// On supprime le service $ docker service rm rng // Puis on le relance en activant l\\\u0026#39;ordonnancement global $ docker service create --name rng --network dockercoins --mode global \\  $REGISTRY/$USER_NAMESPACE/rng:$TAG "},{"uri":"https://zaggash.github.io/tp-iut-docker/docker_linux/","title":"Docker et Linux","tags":[],"description":"","content":"Chapter 4 Docker et Linux Dans cette partie, nous allons voir comment Docker s\u0026rsquo;intègre avec linux au niveau réseau puis la gestion des volumes.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/before_docker/","title":"Avant Docker","tags":[],"description":"","content":"Les applications étaient principalement toutes installées sur des Machines Virtuelles.\nCertaines fois plusieurs applications partagent la même VM avec ses propres librairies, dépendances, fichiers de configurations\u0026hellip;\nLes installations se sont ensuite automatisées avec Ansible, Chef, Puppet,\u0026hellip; mais il est très facile de modifier un fichier directement sur la machine sans changer le template.\nCe qui rend les environnements certaines fois non fiable.\nLes Ops et Dev n\u0026rsquo;ont pas forcement une manière simple de partager les applications.\nLes environnements varient ce qui crée des lenteurs et des frictions entre Dev et Ops.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/orchestration/","title":"Orchestration","tags":[],"description":"","content":"Chapter 5 Orchestration Cette partie met en oeuvre l\u0026rsquo;ensemble de la stack de Docker.\nNous allons utiliser docker-compose dans un premier temps. Puis mettre en oeuvre un cluster Swarm.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/after_docker/","title":"Après Docker","tags":[],"description":"","content":"Les applications sont désormais deployées seules dans une image avec les dependances et configurations.\nDev et Prod peuvent facilement echanger l\u0026rsquo;application et la deployer en Production.\nLes mises à jour ne neccessitent plus une reinstallation mais seulement un changement d\u0026rsquo;image.\nDe la même maniere, il est désormais très simple de revenir à une version précédente.\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/introduction/why_docker/vm_container/","title":"Alors, VM ou Conteneur ?","tags":[],"description":"","content":"La plupart du temps, les conteneurs tournent dans des VMs.\nLes applications profitent des bénéfices de la contenerisation et la flexibilité des VMs.\nFaire tourner des conteneurs dans une machine complétement physique ajoute des problématiques de scalabilitée.\nIl n\u0026rsquo;y a pas de vérité, tout est une question de besoin !\n   VM Conteneur     Lourd, dans l\u0026rsquo;ordre du Giga Léger, dans l\u0026rsquo;ordre du Mega   Overhead de l\u0026rsquo;hyperviseur Performance native de l\u0026rsquo;hôte   Chaque VMs à son propre OS Les conteneurs partagent l\u0026rsquo;OS de l\u0026rsquo;hôte   Virtualisation Hardware Virtualisation de l\u0026rsquo;OS   Démarrage dans l\u0026rsquo;ordre de la minute Démarrage de l\u0026rsquo;ordre de la milliseconde   Isolation complète, donc plus sécrisée Isolation au niveau du processus, potentiellement moins sécurisée    "},{"uri":"https://zaggash.github.io/tp-iut-docker/","title":"Home","tags":[],"description":"","content":"Home Ceci est un TP afin de découvrir Docker, les conteneurs et l\u0026rsquo;orchestration de conteneur.\nJ\u0026rsquo;ai essayé de mettre le plus d\u0026rsquo;informations possible dans les chapitres.\nN\u0026rsquo;hésitez à m\u0026rsquo;interpeller si vous rencontrez un problème, une erreur, ou avez besoin de plus d\u0026rsquo;explications.\nLa documentation sur le site de Docker peut apporter une aide complémentaire.\nAfin de mener à bien le TP, vous avez à votre disposition 3 VMs.\nSeule une sera utile jusqu\u0026rsquo;à la partie concernant l\u0026rsquo;orchestration.\nCeci n\u0026rsquo;a pas pour but de tout expliquer sur les conteneurs, mais fait plutôt office d\u0026rsquo;introduction.\nAmusez-vous !\n"},{"uri":"https://zaggash.github.io/tp-iut-docker/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://zaggash.github.io/tp-iut-docker/tags/","title":"Tags","tags":[],"description":"","content":""}]